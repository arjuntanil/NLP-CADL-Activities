{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqkB-T5Ax-xW",
        "outputId": "c526caf5-6a2a-4561-cbe3-b60464c55801"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m123.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "📌 Original Corpus:\n",
            "1. The stock market crashed due to global uncertainty.\n",
            "2. Natural Language Processing is a key part of Artificial Intelligence.\n",
            "3. Google releases a new AI model to improve search results.\n",
            "4. The weather today is sunny and pleasant in New York.\n",
            "5. Sports events are being postponed because of heavy rains.\n",
            "\n",
            "================ NLTK Preprocessing ================\n",
            "\n",
            "Sentence 1: The stock market crashed due to global uncertainty.\n",
            "👉 Tokens: ['the', 'stock', 'market', 'crashed', 'due', 'to', 'global', 'uncertainty', '.']\n",
            "👉 After Stopword Removal: ['stock', 'market', 'crashed', 'due', 'global', 'uncertainty']\n",
            "👉 After Stemming: ['stock', 'market', 'crash', 'due', 'global', 'uncertainti']\n",
            "👉 After Lemmatization: ['stock', 'market', 'crashed', 'due', 'global', 'uncertainty']\n",
            "\n",
            "Sentence 2: Natural Language Processing is a key part of Artificial Intelligence.\n",
            "👉 Tokens: ['natural', 'language', 'processing', 'is', 'a', 'key', 'part', 'of', 'artificial', 'intelligence', '.']\n",
            "👉 After Stopword Removal: ['natural', 'language', 'processing', 'key', 'part', 'artificial', 'intelligence']\n",
            "👉 After Stemming: ['natur', 'languag', 'process', 'key', 'part', 'artifici', 'intellig']\n",
            "👉 After Lemmatization: ['natural', 'language', 'processing', 'key', 'part', 'artificial', 'intelligence']\n",
            "\n",
            "Sentence 3: Google releases a new AI model to improve search results.\n",
            "👉 Tokens: ['google', 'releases', 'a', 'new', 'ai', 'model', 'to', 'improve', 'search', 'results', '.']\n",
            "👉 After Stopword Removal: ['google', 'releases', 'new', 'ai', 'model', 'improve', 'search', 'results']\n",
            "👉 After Stemming: ['googl', 'releas', 'new', 'ai', 'model', 'improv', 'search', 'result']\n",
            "👉 After Lemmatization: ['google', 'release', 'new', 'ai', 'model', 'improve', 'search', 'result']\n",
            "\n",
            "Sentence 4: The weather today is sunny and pleasant in New York.\n",
            "👉 Tokens: ['the', 'weather', 'today', 'is', 'sunny', 'and', 'pleasant', 'in', 'new', 'york', '.']\n",
            "👉 After Stopword Removal: ['weather', 'today', 'sunny', 'pleasant', 'new', 'york']\n",
            "👉 After Stemming: ['weather', 'today', 'sunni', 'pleasant', 'new', 'york']\n",
            "👉 After Lemmatization: ['weather', 'today', 'sunny', 'pleasant', 'new', 'york']\n",
            "\n",
            "Sentence 5: Sports events are being postponed because of heavy rains.\n",
            "👉 Tokens: ['sports', 'events', 'are', 'being', 'postponed', 'because', 'of', 'heavy', 'rains', '.']\n",
            "👉 After Stopword Removal: ['sports', 'events', 'postponed', 'heavy', 'rains']\n",
            "👉 After Stemming: ['sport', 'event', 'postpon', 'heavi', 'rain']\n",
            "👉 After Lemmatization: ['sport', 'event', 'postponed', 'heavy', 'rain']\n",
            "\n",
            "================ spaCy Preprocessing ================\n",
            "\n",
            "Sentence 1: The stock market crashed due to global uncertainty.\n",
            "👉 Tokens: ['the', 'stock', 'market', 'crashed', 'due', 'to', 'global', 'uncertainty', '.']\n",
            "👉 After Stopword Removal: ['stock', 'market', 'crashed', 'global', 'uncertainty']\n",
            "👉 After Lemmatization: ['stock', 'market', 'crash', 'global', 'uncertainty']\n",
            "\n",
            "Sentence 2: Natural Language Processing is a key part of Artificial Intelligence.\n",
            "👉 Tokens: ['natural', 'language', 'processing', 'is', 'a', 'key', 'part', 'of', 'artificial', 'intelligence', '.']\n",
            "👉 After Stopword Removal: ['natural', 'language', 'processing', 'key', 'artificial', 'intelligence']\n",
            "👉 After Lemmatization: ['natural', 'language', 'processing', 'key', 'artificial', 'intelligence']\n",
            "\n",
            "Sentence 3: Google releases a new AI model to improve search results.\n",
            "👉 Tokens: ['google', 'releases', 'a', 'new', 'ai', 'model', 'to', 'improve', 'search', 'results', '.']\n",
            "👉 After Stopword Removal: ['google', 'releases', 'new', 'ai', 'model', 'improve', 'search', 'results']\n",
            "👉 After Lemmatization: ['google', 'release', 'new', 'ai', 'model', 'improve', 'search', 'result']\n",
            "\n",
            "Sentence 4: The weather today is sunny and pleasant in New York.\n",
            "👉 Tokens: ['the', 'weather', 'today', 'is', 'sunny', 'and', 'pleasant', 'in', 'new', 'york', '.']\n",
            "👉 After Stopword Removal: ['weather', 'today', 'sunny', 'pleasant', 'new', 'york']\n",
            "👉 After Lemmatization: ['weather', 'today', 'sunny', 'pleasant', 'new', 'york']\n",
            "\n",
            "Sentence 5: Sports events are being postponed because of heavy rains.\n",
            "👉 Tokens: ['sports', 'events', 'are', 'being', 'postponed', 'because', 'of', 'heavy', 'rains', '.']\n",
            "👉 After Stopword Removal: ['sports', 'events', 'postponed', 'heavy', 'rains']\n",
            "👉 After Lemmatization: ['sport', 'event', 'postpone', 'heavy', 'rain']\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# NLP Preprocessing in Python\n",
        "# Using both NLTK and spaCy\n",
        "# ================================\n",
        "\n",
        "!pip install nltk spacy\n",
        "\n",
        "# Import & download resources\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')   # ✅ Fix for new NLTK versions\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# ================================\n",
        "# Sample Dataset\n",
        "# ================================\n",
        "corpus = [\n",
        "    \"The stock market crashed due to global uncertainty.\",\n",
        "    \"Natural Language Processing is a key part of Artificial Intelligence.\",\n",
        "    \"Google releases a new AI model to improve search results.\",\n",
        "    \"The weather today is sunny and pleasant in New York.\",\n",
        "    \"Sports events are being postponed because of heavy rains.\"\n",
        "]\n",
        "\n",
        "print(\"📌 Original Corpus:\")\n",
        "for i, doc in enumerate(corpus, 1):\n",
        "    print(f\"{i}. {doc}\")\n",
        "\n",
        "# ================================\n",
        "# 🔹 NLTK Preprocessing\n",
        "# ================================\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"\\n================ NLTK Preprocessing ================\")\n",
        "\n",
        "for i, doc in enumerate(corpus, 1):\n",
        "    tokens = word_tokenize(doc.lower())  # Tokenization\n",
        "    no_stop = [w for w in tokens if w.isalpha() and w not in stop_words]  # Stopword removal\n",
        "    stemmed = [stemmer.stem(w) for w in no_stop]  # Stemming\n",
        "    lemmatized = [lemmatizer.lemmatize(w) for w in no_stop]  # Lemmatization\n",
        "\n",
        "    print(f\"\\nSentence {i}: {doc}\")\n",
        "    print(f\"👉 Tokens: {tokens}\")\n",
        "    print(f\"👉 After Stopword Removal: {no_stop}\")\n",
        "    print(f\"👉 After Stemming: {stemmed}\")\n",
        "    print(f\"👉 After Lemmatization: {lemmatized}\")\n",
        "\n",
        "# ================================\n",
        "# 🔹 spaCy Preprocessing\n",
        "# ================================\n",
        "print(\"\\n================ spaCy Preprocessing ================\")\n",
        "\n",
        "for i, doc in enumerate(corpus, 1):\n",
        "    spacy_doc = nlp(doc.lower())\n",
        "\n",
        "    tokens = [token.text for token in spacy_doc]  # Tokenization\n",
        "    no_stop = [token.text for token in spacy_doc if not token.is_stop and token.is_alpha]  # Stopword removal\n",
        "    lemmatized = [token.lemma_ for token in spacy_doc if not token.is_stop and token.is_alpha]  # Lemmatization\n",
        "\n",
        "    print(f\"\\nSentence {i}: {doc}\")\n",
        "    print(f\"👉 Tokens: {tokens}\")\n",
        "    print(f\"👉 After Stopword Removal: {no_stop}\")\n",
        "    print(f\"👉 After Lemmatization: {lemmatized}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bm9kW4_dyjbT"
      }
    }
  ]
}