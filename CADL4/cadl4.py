# -*- coding: utf-8 -*-
"""CADL4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cit80nYBcRG6egnS9bKGwJHAGj3vHOmf
"""


# Restart runtime automatically after fixing dependencies
import os, sys
os.kill(os.getpid(), 9)

# -------------------------------
# Imports & NLTK downloads
# -------------------------------
import nltk
nltk.download('punkt')
nltk.download('punkt_tab')    
nltk.download('stopwords')
nltk.download('omw-1.4')

import spacy
nlp = spacy.load("en_core_web_sm", disable=['parser','ner'])  # only tagger+lemmatizer

import gensim
from gensim import corpora
from gensim.models import Phrases, LdaModel
from gensim.models.phrases import Phraser

import pyLDAvis
import pyLDAvis.gensim_models as gensimvis

import pandas as pd
import matplotlib.pyplot as plt
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

# -------------------------------
# Sample corpus (social media posts)
# -------------------------------
corpus = [
    "Just tried the new cafe downtown — the coffee was amazing and the ambiance perfect.",
    "Feeling blessed to have completed my first 5K run today! #fitness #goals",
    "Major update: the latest smartphone from TechCorp features a 108MP camera and longer battery life.",
    "Elections next month — make sure to register and vote. Democracy matters.",
    "Watching the championship game tonight, go Tigers! What a comeback in the second half.",
    "I'm finally starting my online data science course. Excited and nervous!",
    "New study shows climate change is accelerating polar ice melt.",
    "Anyone recommends a good budget laptop for programming and gaming?",
    "Baking banana bread during lockdown. Smells so good!",
    "Stock markets rally after positive earnings reports from major banks."
]

# -------------------------------
# 1) Preprocessing function
# -------------------------------
stop_words = set(stopwords.words('english'))

def preprocess_texts(texts, nlp_pipeline):
    processed_texts = []
    for doc in texts:
        tokens = word_tokenize(doc.lower())
        tokens = [t for t in tokens if t.isalpha() and t not in stop_words and len(t) > 2]
        spacy_doc = nlp_pipeline(" ".join(tokens))
        lemmas = [token.lemma_ for token in spacy_doc if token.is_alpha and token.lemma_ not in stop_words and len(token.lemma_) > 2]
        processed_texts.append(lemmas)
    return processed_texts

processed = preprocess_texts(corpus, nlp)
print("Sample preprocessed docs:", processed[:3])

# -------------------------------
# 2) Create bigrams
# -------------------------------
bigram = Phrases(processed, min_count=2, threshold=5)
bigram_mod = Phraser(bigram)
data_bigrams = [bigram_mod[doc] for doc in processed]

# -------------------------------
# 3) Dictionary + Corpus
# -------------------------------
id2word = corpora.Dictionary(data_bigrams)
id2word.filter_extremes(no_below=1, no_above=0.5)
gensim_corpus = [id2word.doc2bow(text) for text in data_bigrams]

# -------------------------------
# 4) LDA model
# -------------------------------
num_topics = 3
lda_model = LdaModel(corpus=gensim_corpus,
                     id2word=id2word,
                     num_topics=num_topics,
                     random_state=100,
                     update_every=1,
                     passes=10,
                     alpha='auto')

print("\nLDA topics:")
for idx, topic in lda_model.print_topics(num_topics=num_topics, num_words=6):
    print(f"Topic {idx}: {topic}")

# -------------------------------
# 5) Interactive visualization
# -------------------------------
pyLDAvis.enable_notebook()
vis = gensimvis.prepare(lda_model, gensim_corpus, id2word)
vis